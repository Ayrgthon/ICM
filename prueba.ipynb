{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vosk in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (0.3.45)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from vosk) (1.17.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from vosk) (2.32.3)\n",
      "Requirement already satisfied: srt in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from vosk) (3.5.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from vosk) (4.67.1)\n",
      "Requirement already satisfied: websockets in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from vosk) (15.0.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from cffi>=1.0->vosk) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from requests->vosk) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from requests->vosk) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from requests->vosk) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from requests->vosk) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from tqdm->vosk) (0.4.6)\n",
      "Requirement already satisfied: pipwin in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: docopt in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from pipwin) (0.6.2)\n",
      "Requirement already satisfied: requests in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from pipwin) (2.32.3)\n",
      "Requirement already satisfied: pyprind in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from pipwin) (2.11.3)\n",
      "Requirement already satisfied: six in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from pipwin) (1.17.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.0 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from pipwin) (4.13.3)\n",
      "Requirement already satisfied: js2py in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from pipwin) (0.74)\n",
      "Requirement already satisfied: packaging in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from pipwin) (24.2)\n",
      "Requirement already satisfied: pySmartDL>=1.3.1 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from pipwin) (1.3.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from beautifulsoup4>=4.9.0->pipwin) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from beautifulsoup4>=4.9.0->pipwin) (4.12.2)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from js2py->pipwin) (5.3.1)\n",
      "Requirement already satisfied: pyjsparser>=2.5.1 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from js2py->pipwin) (2.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from requests->pipwin) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from requests->pipwin) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from requests->pipwin) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from requests->pipwin) (2025.1.31)\n",
      "Requirement already satisfied: tzdata in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (from tzlocal>=1.2->js2py->pipwin) (2025.1)\n",
      "Requirement already satisfied: pyaudio in c:\\users\\sorac\\onedrive\\documents\\aura\\icm\\auraenv\\lib\\site-packages (0.2.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install vosk\n",
    "!pip install pipwin\n",
    "!pip install pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micrófono abierto, habla algo... (Ctrl+C para salir)\n",
      "** Frase reconocida: \n",
      "** Frase reconocida: hola cómo estás\n",
      "** Frase reconocida: \n",
      "** Frase reconocida: actívate\n",
      "Saliendo...\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import json\n",
    "import sys\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "# 1. Cargar el modelo de Vosk\n",
    "model_path = r\"vosk-model-es-0.42\"\n",
    "Model = Model(model_path)\n",
    "\n",
    "# 2. Configurar PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Parám. de audio: 16 kHz, mono, 16 bits\n",
    "RATE = 16000\n",
    "CHANNELS = 1\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHUNK = 4096  # tamaño de bloque\n",
    "\n",
    "# 3. Abrir stream de audio desde el micrófono\n",
    "stream = p.open(\n",
    "    format=FORMAT,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True,\n",
    "    frames_per_buffer=CHUNK\n",
    ")\n",
    "\n",
    "print(\"Micrófono abierto, habla algo... (Ctrl+C para salir)\")\n",
    "\n",
    "# Crear el reconocedor con la misma frecuencia de muestreo\n",
    "rec = KaldiRecognizer(Model, RATE)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # 4. Leer del micrófono en bloques\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        \n",
    "        # 5. Al alimentar el audio al reconocedor, Vosk determina si hay un resultado completo\n",
    "        if rec.AcceptWaveform(data):\n",
    "            # AcceptWaveform => frase terminada\n",
    "            result_json = rec.Result()\n",
    "            result_dict = json.loads(result_json)\n",
    "            text = result_dict.get(\"text\", \"\")\n",
    "            print(\"** Frase reconocida:\", text)\n",
    "        else:\n",
    "            # Caso contrario => partial result\n",
    "            partial_json = rec.PartialResult()\n",
    "            partial_dict = json.loads(partial_json)\n",
    "            partial_text = partial_dict.get(\"partial\", \"\")\n",
    "            # Si quieres ver el texto parcial mientras hablas, descomenta:\n",
    "            # print(\"...partial...\", partial_text)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Saliendo...\")\n",
    "\n",
    "# 6. Cerrar y limpiar\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo Vosk desde: vosk-model-es-0.42\n",
      "Modelo Vosk cargado correctamente.\n",
      "Cargando modelo de Image Captioning desde: ViTGP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de Image Captioning cargado correctamente.\n",
      "Micrófono activado. Di 'describe' para generar descripción, o 'salir' para terminar.\n",
      "Reconocido: describe\n",
      "Generated caption: Está preparando masa para pan o pizza. En la mesa de enfrente hay harina esparcida, un bol grande vacío y una taza medidora, los cuales suelen utilizarse en la preparación de masa para hornear.\n",
      "Reconocido: salir\n",
      "Saliendo de la escucha por micrófono...\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import json\n",
    "import sys\n",
    "from vosk import Model as VoskModel, KaldiRecognizer\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer, ViTImageProcessor\n",
    "\n",
    "#####################\n",
    "# 1. VOSK: CARGAR MODELO\n",
    "#####################\n",
    "def load_vosk_model(model_path=r\"vosk-model-es-0.42\", rate=16000):\n",
    "    \"\"\"\n",
    "    Carga el modelo de Vosk y configura PyAudio para reconocer en vivo.\n",
    "    \"\"\"\n",
    "    print(f\"Cargando modelo Vosk desde: {model_path}\")\n",
    "    vosk_model = VoskModel(model_path)\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    # Ajustes de audio: 16 kHz, mono, 16 bits\n",
    "    RATE = rate\n",
    "    CHANNELS = 1\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHUNK = 4096\n",
    "\n",
    "    stream = p.open(\n",
    "        format=FORMAT,\n",
    "        channels=CHANNELS,\n",
    "        rate=RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK\n",
    "    )\n",
    "\n",
    "    recognizer = KaldiRecognizer(vosk_model, RATE)\n",
    "    return p, stream, recognizer\n",
    "\n",
    "\n",
    "#####################\n",
    "# 2. IMAGE CAPTIONING: CARGAR Y DESCRIBIR\n",
    "#####################\n",
    "def load_captioning_model(model_path=\"ViTGP\", device=\"cpu\"):\n",
    "    print(f\"Cargando modelo de Image Captioning desde: {model_path}\")\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    feature_extractor = ViTImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, feature_extractor\n",
    "\n",
    "def generate_caption(\n",
    "    image_path,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    feature_extractor,\n",
    "    device=\"cpu\",\n",
    "    max_length=30,\n",
    "    num_beams=4\n",
    "):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            pixel_values,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams\n",
    "        )\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "\n",
    "#####################\n",
    "# 3. ESCUCHA POR MICRÓFONO Y PROCESA COMANDOS\n",
    "#####################\n",
    "def listen_microphone(stream, recognizer, model, tokenizer, feature_extractor, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Lee audio del micrófono en un bucle.\n",
    "    - Si Vosk reconoce la palabra 'describe', genera la caption.\n",
    "    - Si reconoce 'salir', rompe el bucle.\n",
    "    \"\"\"\n",
    "    print(\"Micrófono activado. Di 'describe' para generar descripción, o 'salir' para terminar.\")\n",
    "    try:\n",
    "        while True:\n",
    "            data = stream.read(4096, exception_on_overflow=False)\n",
    "            if recognizer.AcceptWaveform(data):\n",
    "                result_json = recognizer.Result()\n",
    "                result_dict = json.loads(result_json)\n",
    "                recognized_text = result_dict.get(\"text\", \"\").strip().lower()\n",
    "\n",
    "                if recognized_text:\n",
    "                    print(f\"Reconocido: {recognized_text}\")\n",
    "\n",
    "                    if recognized_text == \"describe\":\n",
    "                        caption = generate_caption(\n",
    "                            image_path=\"B601743B-02FF-4B4F-B065-3B95473124D6-1030x773.jpeg\",\n",
    "                            model=model,\n",
    "                            tokenizer=tokenizer,\n",
    "                            feature_extractor=feature_extractor,\n",
    "                            device=device,\n",
    "                            max_length=120,\n",
    "                            num_beams=4\n",
    "                        )\n",
    "                        print(\"Generated caption:\", caption)\n",
    "\n",
    "                    elif recognized_text == \"salir\":\n",
    "                        print(\"Saliendo de la escucha por micrófono...\")\n",
    "                        break\n",
    "\n",
    "            # El else atiende partial results, si quieres examinarlos\n",
    "            # else:\n",
    "            #    partial = recognizer.PartialResult()\n",
    "            #    ...\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrumpido con Ctrl+C\")\n",
    "\n",
    "\n",
    "def run_commands(p, stream, recognizer, model, tokenizer, feature_extractor, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Inicia la escucha del micrófono y ejecuta comandos según lo reconocido.\n",
    "    Al terminar, cierra PyAudio.\n",
    "    \"\"\"\n",
    "    listen_microphone(stream, recognizer, model, tokenizer, feature_extractor, device)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "\n",
    "#####################\n",
    "# 4. PUNTO DE ENTRADA PRINCIPAL\n",
    "#####################\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 1) Activar y cargar el modelo de Vosk por defecto\n",
    "    p, stream, recognizer = load_vosk_model(model_path=\"vosk-model-es-0.42\", rate=16000)\n",
    "    print(\"Modelo Vosk cargado correctamente.\")\n",
    "\n",
    "    # 2) Activar y cargar el modelo de Captioning por defecto\n",
    "    model, tokenizer, feature_extractor = load_captioning_model(\n",
    "        model_path=\"ViTGP\",  # Ajustar a tu ruta de fine-tuning\n",
    "        device=device\n",
    "    )\n",
    "    print(\"Modelo de Image Captioning cargado correctamente.\")\n",
    "\n",
    "    # 3) Ejecutar la lógica de comandos por voz\n",
    "    run_commands(p, stream, recognizer, model, tokenizer, feature_extractor, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auraenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
